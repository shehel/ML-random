{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hearth/ML/course/deeplearning1/nbs'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking directory \n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "#reference to imp directories\n",
    "current_dir = os.getcwd()\n",
    "LESSON_HOME_DIR = current_dir\n",
    "DATA_HOME_DIR = current_dir+'/data/statefarm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 7003 on context None\n",
      "Mapped name None to device cuda0: GeForce GTX 1080 Ti (0000:01:00.0)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import utils; importlib.reload(utils)\n",
    "from utils import *\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import vgg16bn; importlib.reload(vgg16bn)\n",
    "from vgg16bn import Vgg16BN\n",
    "vgg = Vgg16BN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hearth/ML/course/deeplearning1/nbs/data/statefarm\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "path = DATA_HOME_DIR+'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Creating directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%mkdir valid\n",
    "%mkdir results\n",
    "%mkdir -p sample/train\n",
    "%mkdir -p sample/test\n",
    "%mkdir -p sample/valid\n",
    "%mkdir -p sample/results\n",
    "%mkdir -p test/unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Creating Validation set based on drivers. \n",
    "Need to have separate drivers in validation and train set to prevent overfitting based on drivers. Drivers aren't important but their actions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,10):\n",
    "    os.mkdir(valid_path+'c'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "drivers = df.groupby('subject').groups.keys()\n",
    "print (drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv('driver_imgs_list.csv')\n",
    "drivers = df.groupby('subject').groups.keys()\n",
    "print (drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "driver = df.loc[df['subject'] == 'p049']\n",
    "for index, row in driver.iterrows():\n",
    "    location = row[1]+'/'\n",
    "    name = row[2]\n",
    "    fname = train_path+location+name\n",
    "    if (os.path.isfile(fname)):\n",
    "        os.rename(train_path+location+name, valid_path+location+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Setting folder locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hearth/ML/course/deeplearning1/nbs/data/statefarm\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "test_path = path + '/test/' #We use all the test data\n",
    "results_path=DATA_HOME_DIR + '/results/'\n",
    "train_path=path + 'train/'\n",
    "valid_path=path + 'valid/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Moving data to appropriate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "paths = glob('*/')\n",
    "print (paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Making validation set and all other sets. Switch between copyfile and os.rename as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i in paths:\n",
    "    tpath = train_path+i\n",
    "    npath = sample_path+i\n",
    "    %mkdir $npath\n",
    "    %cd $tpath\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(1000): copyfile(shuf[i], npath+shuf[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Creating sample test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%cd $fulltest\n",
    "g = glob('*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(30): copyfile(shuf[i], test_path+shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create single 'unknown' class for test set\n",
    "%cd $DATA_HOME_DIR/test\n",
    "%mv *.jpg unknown/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 2Conv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def conv1(batches):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Convolution2D(64,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(batches, steps_per_epoch=int(np.ceil(batches.samples/64)), nb_epoch=1, validation_data=val_batches, \n",
    "                     validation_steps=int(np.ceil(val_batches.samples/64)))\n",
    "    #model.optimizer.lr = 0.001\n",
    "    #model.fit_generator(batches, steps_per_epoch=int(np.ceil(batches.samples/64)), nb_epoch=1, validation_data=val_batches, \n",
    "     #                validation_steps=int(np.ceil(val_batches.samples/64)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def test(self, path, batch_size=8):\n",
    "        print(path)\n",
    "        test_batches = get_batches(self, path, shuffle=False, batch_size=batch_size, class_mode=None)\n",
    "        print (test_batches.samples)\n",
    "        return test_batches, self.model.predict_generator(test_batches, int(np.ceil(test_batches.samples/batch_size)))\n",
    "def get_batches(self, path, gen=image.ImageDataGenerator(), shuffle=False, batch_size=8, class_mode='categorical'):\n",
    "        return gen.flow_from_directory(path, target_size=(224,224),\n",
    "                class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batches, preds = test(bn_model, test_path, batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "batches, preds = vgg.test(test_path, batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "save_array(results_path+'test_preds.dat', preds)\n",
    "save_array(results_path+'filenames.dat', batches.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle is  False\n",
      "Found 18509 images belonging to 10 classes.\n",
      "shuffle is  False\n",
      "Found 3915 images belonging to 10 classes.\n",
      "shuffle is  False\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "batches = get_batches(train_path, batch_size=batch_size)\n",
    "val_batches = get_batches(valid_path, batch_size=batch_size)\n",
    "test_batches = get_batches(test_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = vgg.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Vgg16 without BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = vgg.model\n",
    "# Here, [-1] means that, fetch the last element from the array returned by the for loop\n",
    "#last_conv_idx = [i for i, l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "#conv_layers = model.layers[:last_conv_idx+1]\n",
    "#conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shuffle is  False\n",
      "Found 18509 images belonging to 10 classes.\n",
      "shuffle is  False\n",
      "Found 3915 images belonging to 10 classes.\n",
      "shuffle is  False\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $results_path\n",
    "rm -R conv*.dat\n",
    "%cd $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv_feat2 = model.predict_generator(batches, np.ceil(batches.samples/batch_size))\n",
    "#conv_val_feat2 = model.predict_generator(val_batches, np.ceil(val_batches.samples/batch_size))\n",
    "#conv_test_feat = conv_model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreCalculating convolution features for train, val and test data to speed up computations. The precalculated features are stored in bcolz array [due to shortage of RAM] and written to disk to free up RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = results_path+'conv_featFinalxx.dat'\n",
    "for i in range(batches.n // batch_size+1):\n",
    "    conv_feat = model.predict_on_batch(batches.next()[0])\n",
    "    if not i:\n",
    "        c = bcolz.carray(conv_feat, rootdir=fname, mode='a')\n",
    "    else:\n",
    "        c.append(conv_feat)\n",
    "    c.shape\n",
    "c.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = results_path+'conv_val_featFinalxx.dat'\n",
    "for i in range(val_batches.n // batch_size+1):\n",
    "    conv_val_feat = model.predict_on_batch(val_batches.next()[0])\n",
    "    if not i:\n",
    "        c = bcolz.carray(conv_val_feat, rootdir=fname, mode='a')\n",
    "    else:\n",
    "        c.append(conv_val_feat)\n",
    "c.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = results_path+'conv_test_featFinalxx.dat'\n",
    "for i in range(test_batches.n // batch_size+1):\n",
    "    conv_test_feat = conv_model.predict_on_batch(test_batches.next()[0])\n",
    "    if not i:\n",
    "        c = bcolz.carray(conv_test_feat, rootdir=fname, mode='a')\n",
    "    else:\n",
    "        c.append(conv_test_feat)\n",
    "c.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv_val_feat = load_array(path+'results/conv_val_feat.dat')\n",
    "#conv_val_feat2 = load_array(path+'results/conv_val_y.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = results_path+'conv_val_yFinalxx.dat'\n",
    "save_array(fname, val_labels)\n",
    "fname = results_path+'conv_train_yFinalxx.dat'\n",
    "save_array(fname, trn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post pre-calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=bcolz.open(results_path+'conv_featFinalxx.dat', mode='r')\n",
    "y= bcolz.open(results_path+'conv_train_yFinalxx.dat', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valX=bcolz.open(results_path+'conv_val_featFinalxx.dat', mode='r')\n",
    "valy= bcolz.open(results_path+'conv_val_yFinalxx.dat', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_batches=BcolzArrayIterator(X,y, batch_size=batch_size, shuffle=True)\n",
    "vl_batches=BcolzArrayIterator(valX,valy, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testX=bcolz.open(path+'results/conv_test_featFinal.dat', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_batches=BcolzArrayIterator(testX, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = bn_model.predict(conv_val_feat, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense layers on pretrained conv layers\n",
    "Edit: VGG with Batchnorm using batchnorm-vgg trained model and weights. http://files.fast.ai/models/\n",
    "\n",
    "Train: 18509\n",
    "Validate: 3915\n",
    "Test:  79726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18509"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_batches.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=model.output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p/2),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/keras/models.py:852: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8458 samples, validate on 1542 samples\n",
      "Epoch 1/30\n",
      "8458/8458 [==============================] - 0s - loss: 2.5027 - acc: 0.3886 - val_loss: 1.1049 - val_acc: 0.6608\n",
      "Epoch 2/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.7333 - acc: 0.7609 - val_loss: 1.0143 - val_acc: 0.6874\n",
      "Epoch 3/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.4042 - acc: 0.8669 - val_loss: 1.0613 - val_acc: 0.6900\n",
      "Epoch 4/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.2674 - acc: 0.9136 - val_loss: 0.9952 - val_acc: 0.7237\n",
      "Epoch 5/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.2115 - acc: 0.9325 - val_loss: 0.9724 - val_acc: 0.7023\n",
      "Epoch 6/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.1611 - acc: 0.9492 - val_loss: 0.8651 - val_acc: 0.7387\n",
      "Epoch 7/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.1307 - acc: 0.9591 - val_loss: 0.9541 - val_acc: 0.7224\n",
      "Epoch 8/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.1245 - acc: 0.9605 - val_loss: 0.9244 - val_acc: 0.7302\n",
      "Epoch 9/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0908 - acc: 0.9709 - val_loss: 0.9220 - val_acc: 0.7263\n",
      "Epoch 10/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.1026 - acc: 0.9677 - val_loss: 1.0391 - val_acc: 0.7121\n",
      "Epoch 11/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0830 - acc: 0.9735 - val_loss: 0.8399 - val_acc: 0.7445\n",
      "Epoch 12/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0792 - acc: 0.9761 - val_loss: 0.9242 - val_acc: 0.7250\n",
      "Epoch 13/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0741 - acc: 0.9778 - val_loss: 0.8759 - val_acc: 0.7270\n",
      "Epoch 14/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0783 - acc: 0.9762 - val_loss: 0.9891 - val_acc: 0.7283\n",
      "Epoch 15/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0652 - acc: 0.9804 - val_loss: 1.1408 - val_acc: 0.7023\n",
      "Epoch 16/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0788 - acc: 0.9767 - val_loss: 1.1398 - val_acc: 0.7043\n",
      "Epoch 17/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0752 - acc: 0.9765 - val_loss: 1.1023 - val_acc: 0.7069\n",
      "Epoch 18/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0579 - acc: 0.9817 - val_loss: 1.0938 - val_acc: 0.7114\n",
      "Epoch 19/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0673 - acc: 0.9773 - val_loss: 1.2199 - val_acc: 0.6764\n",
      "Epoch 20/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0691 - acc: 0.9758 - val_loss: 1.2080 - val_acc: 0.6861\n",
      "Epoch 21/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0584 - acc: 0.9798 - val_loss: 1.1640 - val_acc: 0.7082\n",
      "Epoch 22/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0498 - acc: 0.9837 - val_loss: 1.3996 - val_acc: 0.6861\n",
      "Epoch 23/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0538 - acc: 0.9826 - val_loss: 1.1106 - val_acc: 0.7302\n",
      "Epoch 24/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0509 - acc: 0.9831 - val_loss: 1.1722 - val_acc: 0.7134\n",
      "Epoch 25/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0621 - acc: 0.9826 - val_loss: 1.3793 - val_acc: 0.6770\n",
      "Epoch 26/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0626 - acc: 0.9810 - val_loss: 1.5213 - val_acc: 0.6790\n",
      "Epoch 27/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0598 - acc: 0.9817 - val_loss: 1.4110 - val_acc: 0.6861\n",
      "Epoch 28/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0541 - acc: 0.9826 - val_loss: 1.2861 - val_acc: 0.7062\n",
      "Epoch 29/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0509 - acc: 0.9838 - val_loss: 1.4031 - val_acc: 0.6803\n",
      "Epoch 30/30\n",
      "8458/8458 [==============================] - 0s - loss: 0.0423 - acc: 0.9858 - val_loss: 1.4010 - val_acc: 0.6926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1d64a330f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit(conv_feat2, trn_labels, batch_size=batch_size, nb_epoch=30, \n",
    "             validation_data=(conv_val_feat2, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<utils.Bco..., steps_per_epoch=290.0, validation_data=<utils.Bco..., validation_steps=62, epochs=10)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "290/290 [==============================] - 2s - loss: 1.6389 - acc: 0.5706 - val_loss: 0.4907 - val_acc: 0.8429\n",
      "Epoch 2/10\n",
      "290/290 [==============================] - 2s - loss: 0.3833 - acc: 0.8753 - val_loss: 0.4843 - val_acc: 0.8293\n",
      "Epoch 3/10\n",
      "290/290 [==============================] - 2s - loss: 0.2324 - acc: 0.9274 - val_loss: 0.4654 - val_acc: 0.8437\n",
      "Epoch 4/10\n",
      "290/290 [==============================] - 2s - loss: 0.1753 - acc: 0.9456 - val_loss: 0.5441 - val_acc: 0.8427\n",
      "Epoch 5/10\n",
      "290/290 [==============================] - 2s - loss: 0.1465 - acc: 0.9544 - val_loss: 0.5108 - val_acc: 0.8381\n",
      "Epoch 6/10\n",
      "290/290 [==============================] - 2s - loss: 0.1270 - acc: 0.9589 - val_loss: 0.3955 - val_acc: 0.8559\n",
      "Epoch 7/10\n",
      "290/290 [==============================] - 2s - loss: 0.1145 - acc: 0.9633 - val_loss: 0.4436 - val_acc: 0.8590\n",
      "Epoch 8/10\n",
      "290/290 [==============================] - 2s - loss: 0.1030 - acc: 0.9671 - val_loss: 0.4591 - val_acc: 0.8682\n",
      "Epoch 9/10\n",
      "290/290 [==============================] - 2s - loss: 0.0999 - acc: 0.9692 - val_loss: 0.6225 - val_acc: 0.8238\n",
      "Epoch 10/10\n",
      "290/290 [==============================] - 2s - loss: 0.0928 - acc: 0.9713 - val_loss: 0.5422 - val_acc: 0.8375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f00f6cf9cf8>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit_generator(trn_batches, nb_epoch=10, steps_per_epoch=np.ceil(trn_batches.N/batch_size), validation_data=vl_batches, \n",
    "                     validation_steps=int(np.ceil(vl_batches.N/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.01\n",
    "bn_model.fit_generator(trn_batches, nb_epoch=10, steps_per_epoch=np.ceil(trn_batches.N/batch_size), validation_data=vl_batches, \n",
    "                     validation_steps=int(np.ceil(vl_batches.N/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'models/conv8_1Final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pseudo = bn_model.predict_generator(tst_batches, steps=np.ceil(tst_batches.N/batch_size), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "test_pseudo[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_pseudo = np.concatenate([trn_labels, test_pseudo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1=bcolz.open(path+'results/conv_featFinal.dat', mode='r')\n",
    "trn_batches=BcolzArrayIterator(X1, batch_size=batch_size, shuffle=False)\n",
    "trn_batches.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X2=bcolz.open(path+'results/conv_test_featFinal.dat', mode='r')\n",
    "tst_batches=BcolzArrayIterator(X2, batch_size=batch_size, shuffle=False)\n",
    "tst_batches.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = path+'results/conv_feat_extFinal.dat'\n",
    "batch = trn_batches.next()\n",
    "c = bcolz.carray(batch, rootdir=fname, mode='w')\n",
    "for i in range(trn_batches.N // batch_size):\n",
    "    batch = trn_batches.next()\n",
    "    c.append(batch)\n",
    "for i in range(tst_batches.N // batch_size+1):\n",
    "    batch = tst_batches.next()\n",
    "    c.append(batch)\n",
    "c.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X3=bcolz.open(path+'results/conv_feat_extFinal.dat', mode='r')\n",
    "\n",
    "X3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = path+'results/conv_ptest_yFinal.dat'\n",
    "save_array(fname, comb_pseudo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_pseudo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre calculated features and train on train+test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pX=bcolz.open(path+'results/conv_feat_extFinal.dat', mode='r')\n",
    "py= bcolz.open(path+'results/conv_ptest_yFinal.dat', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tstComb_batches=BcolzArrayIterator(pX, py, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.load_weights(path+'models/conv8_1Final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.001\n",
    "bn_model.fit_generator(tstComb_batches, nb_epoch=10, steps_per_epoch=np.ceil(tstComb_batches.N/batch_size), validation_data=vl_batches, \n",
    "                     validation_steps=int(np.ceil(vl_batches.N/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.optimizer.lr=0.01\n",
    "bn_model.fit_generator(tstComb_batches, nb_epoch=10, steps_per_epoch=np.ceil(tstComb_batches.N/batch_size), validation_data=vl_batches, \n",
    "                     validation_steps=int(np.ceil(vl_batches.N/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keras.metrics.categorical_crossentropy(val_labels, do_clip(preds, 0.9050)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pX=bcolz.open(path+'results/conv_test_feat.dat', mode='r')\n",
    "tst_batches=BcolzArrayIterator(pX, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred = bn_model.predict_generator(tst_batches, steps=np.ceil(tst_batches.N/batch_size), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = do_clip(test_pred,0.8840)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = sorted(batches.class_indices, key=batches.class_indices.get)\n",
    "subm_name = path+'results/subm2.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'img', [a[8:] for a in test_filenames])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(subm_name, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FileLink(subm_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making submissions, old method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = val_batches.filenames\n",
    "expected_labels = val_batches.classes #0 or 1\n",
    "\n",
    "#Round our predictions to 0/1 to generate labels\n",
    "print (filenames[1], probs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = load_array(results_path + 'test_preds.dat')\n",
    "filenames = load_array(results_path + 'filenames.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = vgg.get_batches(test_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "filenames = batches.filenames\n",
    "a = []\n",
    "\n",
    "for idx, val in enumerate(filenames):\n",
    "    a.append(val[8:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = sorted(val_batches.class_indices, key=val_batches.class_indices.get)\n",
    "submission = pd.DataFrame(preds, columns=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (submission)\n",
    "submission['img']=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = submission.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "submission= submission[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('firstSubVGG', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = load_array('/home/hearth/ML/course/courses/deeplearning1/nbs/data/dogscats/' + 'test_preds.dat')\n",
    "filenames = load_array('/home/hearth/ML/course/courses/deeplearning1/nbs/data/dogscats/' + 'filenames.dat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
