{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/hearth/ML/course/deeplearning1/nbs'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking directory \n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "#reference to imp directories\n",
    "current_dir = os.getcwd()\n",
    "LESSON_HOME_DIR = current_dir\n",
    "DATA_HOME_DIR = current_dir+'/data/statefarm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 7003 on context None\n",
      "Mapped name None to device cuda0: GeForce GTX 1080 Ti (0000:01:00.0)\n",
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import utils; importlib.reload(utils)\n",
    "from utils import *\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Validation and Sample sets and the appropriate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hearth/ML/course/deeplearning1/nbs/vgg16_bn.h5\n"
     ]
    }
   ],
   "source": [
    "import vgg16; importlib.reload(vgg16)\n",
    "from vgg16 import Vgg16\n",
    "vgg = Vgg16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hearth/ML/course/deeplearning1/nbs/data/statefarm\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "path = DATA_HOME_DIR+'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating directory structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mkdir valid\n",
    "%mkdir results\n",
    "%mkdir -p sample/train\n",
    "%mkdir -p sample/test\n",
    "%mkdir -p sample/valid\n",
    "%mkdir -p sample/results\n",
    "%mkdir -p test/unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Validation set based on drivers. \n",
    "Need to have separate drivers in validation and train set to prevent overfitting based on drivers. Drivers aren't important but their actions are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d1642ec9bbb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_path' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    os.mkdir(valid_path+'c'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drivers = df.groupby('subject').groups.keys()\n",
    "print (drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df= pd.read_csv('driver_imgs_list.csv')\n",
    "drivers = df.groupby('subject').groups.keys()\n",
    "print (drivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "driver = df.loc[df['subject'] == 'p049']\n",
    "for index, row in driver.iterrows():\n",
    "    location = row[1]+'/'\n",
    "    name = row[2]\n",
    "    fname = train_path+location+name\n",
    "    if (os.path.isfile(fname)):\n",
    "        os.rename(train_path+location+name, valid_path+location+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting folder locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hearth/ML/course/deeplearning1/nbs/data/statefarm\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from shutil import copyfile\n",
    "test_path = path + '/test/' #We use all the test data\n",
    "results_path=DATA_HOME_DIR + '/results/'\n",
    "train_path=path + 'train/'\n",
    "valid_path=path + 'valid/'\n",
    "fulltrain = DATA_HOME_DIR+'/train/'\n",
    "fulltest = DATA_HOME_DIR+'/test/'\n",
    "sample_path = path+'/sample/train/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving data to appropriate directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $DATA_HOME_DIR/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "paths = glob('*/')\n",
    "print (paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making validation set and all other sets. Switch between copyfile and os.rename as appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in paths:\n",
    "    tpath = train_path+i\n",
    "    npath = sample_path+i\n",
    "    %mkdir $npath\n",
    "    %cd $tpath\n",
    "    g = glob('*.jpg')\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(1000): copyfile(shuf[i], npath+shuf[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating sample test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%cd $fulltest\n",
    "g = glob('*.jpg')\n",
    "shuf = np.random.permutation(g)\n",
    "for i in range(30): copyfile(shuf[i], test_path+shuf[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create single 'unknown' class for test set\n",
    "%cd $DATA_HOME_DIR/test\n",
    "%mv *.jpg unknown/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The above are initialization steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hearth/ML/course/deeplearning1/nbs/data/statefarm\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "\n",
    "#Set path to sample/ path if desired\n",
    "\n",
    "test_path = DATA_HOME_DIR + '/test/' #We use all the test data\n",
    "results_path=DATA_HOME_DIR + '/results/'\n",
    "train_path=path + '/train/'\n",
    "valid_path=path + '/valid/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18509 images belonging to 10 classes.\n",
      "Found 3915 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "batches = vgg.get_batches(path+'train', batch_size=batch_size)\n",
    "val_batches = vgg.get_batches(path+'valid', batch_size=batch_size)\n",
    "test_batches = vgg.get_batches(path+'test', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Conv model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv1(batches):\n",
    "    model = Sequential([\n",
    "            BatchNormalization(axis=1, input_shape=(3,224,224)),\n",
    "            Convolution2D(32,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Convolution2D(64,3,3, activation='relu'),\n",
    "            BatchNormalization(axis=1),\n",
    "            MaxPooling2D((3,3)),\n",
    "            Flatten(),\n",
    "            Dense(200, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "    model.compile(Adam(lr=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit_generator(batches, steps_per_epoch=int(np.ceil(batches.samples/64)), nb_epoch=1, validation_data=val_batches, \n",
    "                     validation_steps=int(np.ceil(val_batches.samples/64)))\n",
    "    #model.optimizer.lr = 0.001\n",
    "    #model.fit_generator(batches, steps_per_epoch=int(np.ceil(batches.samples/64)), nb_epoch=1, validation_data=val_batches, \n",
    "     #                validation_steps=int(np.ceil(val_batches.samples/64)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = conv1(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(self, path, batch_size=8):\n",
    "        print(path)\n",
    "        test_batches = get_batches(self, path, shuffle=False, batch_size=batch_size, class_mode=None)\n",
    "        print (test_batches.samples)\n",
    "        return test_batches, self.model.predict_generator(test_batches, int(np.ceil(test_batches.samples/batch_size)))\n",
    "def get_batches(self, path, gen=image.ImageDataGenerator(), shuffle=False, batch_size=8, class_mode='categorical'):\n",
    "        return gen.flow_from_directory(path, target_size=(224,224),\n",
    "                class_mode=class_mode, shuffle=shuffle, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches, preds = test(bn_model, test_path, batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches, preds = vgg.test(test_path, batch_size = batch_size*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_array(results_path+'test_preds.dat', preds)\n",
    "save_array(results_path+'filenames.dat', batches.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = vgg.model\n",
    "# Here, [-1] means that, fetch the last element from the array returned by the for loop\n",
    "last_conv_idx = [i for i, l in enumerate(model.layers) if type(l) is Convolution2D][-1]\n",
    "conv_layers = model.layers[:last_conv_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_model = Sequential(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18509 images belonging to 10 classes.\n",
      "Found 3915 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    "    val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hearth/ML/course/deeplearning1/nbs/data/statefarm/results\n",
      "/home/hearth/ML/course/deeplearning1/nbs/data/statefarm\n"
     ]
    }
   ],
   "source": [
    "%cd $results_path\n",
    "rm -R conv*.dat\n",
    "%cd $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv_feat2 = conv_model.predict_generator(batches, np.ceil(batches.samples/batch_size))\n",
    "conv_val_feat2 = conv_model.predict_generator(val_batches, np.ceil(val_batches.samples/batch_size))\n",
    "#conv_test_feat = conv_model.predict_generator(test_batches, test_batches.nb_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreCalculating convolution features for train, val and test data to speed up computations. The precalculated features are stored in bcolz array and written to disk to free up RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = results_path+'conv_featFinal.dat'\n",
    "for i in range(batches.n // batch_size+1):\n",
    "    conv_feat = conv_model.predict_on_batch(batches.next()[0])\n",
    "    if not i:\n",
    "        c = bcolz.carray(conv_feat, rootdir=fname, mode='a')\n",
    "    else:\n",
    "        c.append(conv_feat)\n",
    "    c.shape\n",
    "c.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = results_path+'conv_val_featFinal.dat'\n",
    "for i in range(val_batches.n // batch_size+1):\n",
    "    conv_val_feat = conv_model.predict_on_batch(val_batches.next()[0])\n",
    "    if not i:\n",
    "        c = bcolz.carray(conv_val_feat, rootdir=fname, mode='a')\n",
    "    else:\n",
    "        c.append(conv_val_feat)\n",
    "c.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = results_path+'conv_test_featFinal.dat'\n",
    "for i in range(test_batches.n // batch_size+1):\n",
    "    conv_test_feat = conv_model.predict_on_batch(test_batches.next()[0])\n",
    "    if not i:\n",
    "        c = bcolz.carray(conv_test_feat, rootdir=fname, mode='a')\n",
    "    else:\n",
    "        c.append(conv_test_feat)\n",
    "c.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#conv_val_feat = load_array(path+'results/conv_val_feat.dat')\n",
    "#conv_val_feat2 = load_array(path+'results/conv_val_y.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = results_path+'conv_val_yFinal.dat'\n",
    "save_array(fname, val_labels)\n",
    "fname = results_path+'conv_train_yFinal.dat'\n",
    "save_array(fname, trn_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post pre-calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=bcolz.open(results_path+'conv_featFinal.dat', mode='r')\n",
    "y= bcolz.open(results_path+'conv_train_yFinal.dat', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valX=bcolz.open(results_path+'conv_val_featFinal.dat', mode='r')\n",
    "valy= bcolz.open(results_path+'conv_val_yFinal.dat', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn_batches=BcolzArrayIterator(X,y, batch_size=batch_size, shuffle=True)\n",
    "vl_batches=BcolzArrayIterator(valX,valy, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testX=bcolz.open(path+'results/conv_test_featFinal.dat', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tst_batches=BcolzArrayIterator(testX, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = bn_model.predict(conv_val_feat, batch_size=batch_size*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense layers on pretrained conv layers\n",
    "Edit: VGG with Batchnorm using batchnorm-vgg trained model and weights. http://files.fast.ai/models/\n",
    "\n",
    "Train: 18509\n",
    "Validate: 3915\n",
    "Test:  79726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18509"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_batches.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_bn_layers(p):\n",
    "    return [\n",
    "        MaxPooling2D(input_shape=conv_layers[-1].output_shape[1:]),\n",
    "        Flatten(),\n",
    "        Dropout(p/2),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p/2),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(p),\n",
    "        Dense(10, activation='softmax')\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=0.8\n",
    "#print (trn_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model = Sequential(get_bn_layers(p))\n",
    "bn_model.compile(Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##bn_model.fit(conv_feat2, trn_labels, batch_size=batch_size, nb_epoch=10, \n",
    "  ##           validation_data=(conv_val_feat2, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \n",
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<utils.Bco..., steps_per_epoch=290.0, validation_data=<utils.Bco..., validation_steps=62, epochs=10)`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "290/290 [==============================] - 19s - loss: 1.6574 - acc: 0.5515 - val_loss: 0.5802 - val_acc: 0.8097\n",
      "Epoch 2/10\n",
      "290/290 [==============================] - 5s - loss: 0.3183 - acc: 0.9004 - val_loss: 0.5459 - val_acc: 0.8322\n",
      "Epoch 3/10\n",
      "290/290 [==============================] - 5s - loss: 0.1556 - acc: 0.9560 - val_loss: 0.4264 - val_acc: 0.8662\n",
      "Epoch 4/10\n",
      "290/290 [==============================] - 5s - loss: 0.0997 - acc: 0.9726 - val_loss: 0.4577 - val_acc: 0.8720\n",
      "Epoch 5/10\n",
      "290/290 [==============================] - 5s - loss: 0.0700 - acc: 0.9812 - val_loss: 0.4321 - val_acc: 0.8544\n",
      "Epoch 6/10\n",
      "290/290 [==============================] - 5s - loss: 0.0531 - acc: 0.9863 - val_loss: 0.4162 - val_acc: 0.8639\n",
      "Epoch 7/10\n",
      "290/290 [==============================] - 5s - loss: 0.0458 - acc: 0.9877 - val_loss: 0.4810 - val_acc: 0.8447\n",
      "Epoch 8/10\n",
      "290/290 [==============================] - 5s - loss: 0.0483 - acc: 0.9865 - val_loss: 0.4169 - val_acc: 0.8815\n",
      "Epoch 9/10\n",
      "290/290 [==============================] - 5s - loss: 0.0391 - acc: 0.9890 - val_loss: 0.5200 - val_acc: 0.8383\n",
      "Epoch 10/10\n",
      "290/290 [==============================] - 5s - loss: 0.0344 - acc: 0.9900 - val_loss: 0.7637 - val_acc: 0.8036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7feac0ba3358>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.fit_generator(trn_batches, nb_epoch=10, steps_per_epoch=np.ceil(trn_batches.N/batch_size), validation_data=vl_batches, \n",
    "                     validation_steps=int(np.ceil(vl_batches.N/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  9/290 [..............................] - ETA: 4s - loss: 0.0253 - acc: 0.9948"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<utils.Bco..., steps_per_epoch=290.0, validation_data=<utils.Bco..., validation_steps=62, epochs=10)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290/290 [==============================] - 5s - loss: 0.0437 - acc: 0.9866 - val_loss: 0.6854 - val_acc: 0.8245\n",
      "Epoch 2/10\n",
      "290/290 [==============================] - 5s - loss: 0.0361 - acc: 0.9895 - val_loss: 0.7715 - val_acc: 0.8247\n",
      "Epoch 3/10\n",
      "290/290 [==============================] - 5s - loss: 0.0281 - acc: 0.9919 - val_loss: 0.5185 - val_acc: 0.8606\n",
      "Epoch 4/10\n",
      "290/290 [==============================] - 5s - loss: 0.0254 - acc: 0.9922 - val_loss: 0.7761 - val_acc: 0.8291\n",
      "Epoch 5/10\n",
      "290/290 [==============================] - 5s - loss: 0.0272 - acc: 0.9923 - val_loss: 0.4671 - val_acc: 0.8733\n",
      "Epoch 6/10\n",
      "290/290 [==============================] - 5s - loss: 0.0270 - acc: 0.9919 - val_loss: 0.8083 - val_acc: 0.8301\n",
      "Epoch 7/10\n",
      "290/290 [==============================] - 5s - loss: 0.0256 - acc: 0.9927 - val_loss: 0.7623 - val_acc: 0.8365\n",
      "Epoch 8/10\n",
      "290/290 [==============================] - 5s - loss: 0.0217 - acc: 0.9938 - val_loss: 0.7086 - val_acc: 0.8181\n",
      "Epoch 9/10\n",
      "290/290 [==============================] - 5s - loss: 0.0243 - acc: 0.9924 - val_loss: 0.8931 - val_acc: 0.8225\n",
      "Epoch 10/10\n",
      "290/290 [==============================] - 5s - loss: 0.0307 - acc: 0.9903 - val_loss: 0.5087 - val_acc: 0.8738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9ff7e6a58>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.optimizer.lr=0.01\n",
    "bn_model.fit_generator(trn_batches, nb_epoch=10, steps_per_epoch=np.ceil(trn_batches.N/batch_size), validation_data=vl_batches, \n",
    "                     validation_steps=int(np.ceil(vl_batches.N/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.save_weights(path+'models/conv8_1Final.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1243/1246 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "test_pseudo = bn_model.predict_generator(tst_batches, steps=np.ceil(tst_batches.N/batch_size), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "test_pseudo[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_pseudo = np.concatenate([trn_labels, test_pseudo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18509"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1=bcolz.open(path+'results/conv_featFinal.dat', mode='r')\n",
    "trn_batches=BcolzArrayIterator(X1, batch_size=batch_size, shuffle=False)\n",
    "trn_batches.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79726"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2=bcolz.open(path+'results/conv_test_featFinal.dat', mode='r')\n",
    "tst_batches=BcolzArrayIterator(X2, batch_size=batch_size, shuffle=False)\n",
    "tst_batches.N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = path+'results/conv_feat_extFinal.dat'\n",
    "batch = trn_batches.next()\n",
    "c = bcolz.carray(batch, rootdir=fname, mode='w')\n",
    "for i in range(trn_batches.N // batch_size):\n",
    "    batch = trn_batches.next()\n",
    "    c.append(batch)\n",
    "for i in range(tst_batches.N // batch_size+1):\n",
    "    batch = tst_batches.next()\n",
    "    c.append(batch)\n",
    "c.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98235, 512, 14, 14)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X3=bcolz.open(path+'results/conv_feat_extFinal.dat', mode='r')\n",
    "\n",
    "X3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = path+'results/conv_ptest_yFinal.dat'\n",
    "save_array(fname, comb_pseudo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(98235, 10)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comb_pseudo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre calculated features and train on train+test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pX=bcolz.open(path+'results/conv_feat_extFinal.dat', mode='r')\n",
    "py= bcolz.open(path+'results/conv_ptest_yFinal.dat', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tstComb_batches=BcolzArrayIterator(pX, py, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model.load_weights(path+'models/conv8_1Final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   5/1535 [..............................] - ETA: 51s - loss: 1.2713 - acc: 0.7906"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<utils.Bco..., steps_per_epoch=1535.0, validation_data=<utils.Bco..., validation_steps=62, epochs=10)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1535/1535 [==============================] - 76s - loss: 0.5719 - acc: 0.8569 - val_loss: 0.3865 - val_acc: 0.8771\n",
      "Epoch 2/10\n",
      "1535/1535 [==============================] - 32s - loss: 0.4419 - acc: 0.8912 - val_loss: 0.3790 - val_acc: 0.8932\n",
      "Epoch 3/10\n",
      "1535/1535 [==============================] - 26s - loss: 0.4205 - acc: 0.8975 - val_loss: 0.4173 - val_acc: 0.8748\n",
      "Epoch 4/10\n",
      "1535/1535 [==============================] - 26s - loss: 0.4081 - acc: 0.9023 - val_loss: 0.3956 - val_acc: 0.8600\n",
      "Epoch 5/10\n",
      "1535/1535 [==============================] - 27s - loss: 0.3963 - acc: 0.9071 - val_loss: 0.3398 - val_acc: 0.8912\n",
      "Epoch 6/10\n",
      "1535/1535 [==============================] - 27s - loss: 0.3882 - acc: 0.9083 - val_loss: 0.4123 - val_acc: 0.8718\n",
      "Epoch 7/10\n",
      "1535/1535 [==============================] - 26s - loss: 0.3810 - acc: 0.9118 - val_loss: 0.3861 - val_acc: 0.8866\n",
      "Epoch 8/10\n",
      "1535/1535 [==============================] - 26s - loss: 0.3749 - acc: 0.9139 - val_loss: 0.3807 - val_acc: 0.8713\n",
      "Epoch 9/10\n",
      "1535/1535 [==============================] - 27s - loss: 0.3715 - acc: 0.9153 - val_loss: 0.4026 - val_acc: 0.8748\n",
      "Epoch 10/10\n",
      "1535/1535 [==============================] - 26s - loss: 0.3689 - acc: 0.9151 - val_loss: 0.3928 - val_acc: 0.8853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9fd44b518>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.optimizer.lr=0.001\n",
    "bn_model.fit_generator(tstComb_batches, nb_epoch=10, steps_per_epoch=np.ceil(tstComb_batches.N/batch_size), validation_data=vl_batches, \n",
    "                     validation_steps=int(np.ceil(vl_batches.N/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   8/1535 [..............................] - ETA: 28s - loss: 0.3279 - acc: 0.9336"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<utils.Bco..., steps_per_epoch=1535.0, validation_data=<utils.Bco..., validation_steps=62, epochs=10)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1535/1535 [==============================] - 26s - loss: 0.3439 - acc: 0.9251 - val_loss: 0.3923 - val_acc: 0.8841\n",
      "Epoch 2/10\n",
      "1535/1535 [==============================] - 26s - loss: 0.3439 - acc: 0.9249 - val_loss: 0.3955 - val_acc: 0.8689\n",
      "Epoch 3/10\n",
      "1535/1535 [==============================] - 26s - loss: 0.3429 - acc: 0.9260 - val_loss: 0.4050 - val_acc: 0.8674\n",
      "Epoch 4/10\n",
      "1535/1535 [==============================] - 26s - loss: 0.3410 - acc: 0.9261 - val_loss: 0.3645 - val_acc: 0.8807\n",
      "Epoch 5/10\n",
      "1535/1535 [==============================] - 26s - loss: 0.3412 - acc: 0.9268 - val_loss: 0.4082 - val_acc: 0.8705\n",
      "Epoch 6/10\n",
      "1535/1535 [==============================] - 26s - loss: 0.3383 - acc: 0.9284 - val_loss: 0.3822 - val_acc: 0.8874\n",
      "Epoch 7/10\n",
      "1535/1535 [==============================] - 26s - loss: 0.3378 - acc: 0.9291 - val_loss: 0.3906 - val_acc: 0.8769\n",
      "Epoch 8/10\n",
      "1535/1535 [==============================] - 26s - loss: 0.3370 - acc: 0.9279 - val_loss: 0.3867 - val_acc: 0.8848\n",
      "Epoch 9/10\n",
      "1535/1535 [==============================] - 26s - loss: 0.3371 - acc: 0.9279 - val_loss: 0.3634 - val_acc: 0.8894\n",
      "Epoch 10/10\n",
      "1535/1535 [==============================] - 26s - loss: 0.3352 - acc: 0.9283 - val_loss: 0.3739 - val_acc: 0.8840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe9fd44b828>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model.optimizer.lr=0.01\n",
    "bn_model.fit_generator(tstComb_batches, nb_epoch=10, steps_per_epoch=np.ceil(tstComb_batches.N/batch_size), validation_data=vl_batches, \n",
    "                     validation_steps=int(np.ceil(vl_batches.N/batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1-mx)/9, mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-f2e4aac37007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_clip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9050\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preds' is not defined"
     ]
    }
   ],
   "source": [
    "keras.metrics.categorical_crossentropy(val_labels, do_clip(preds, 0.9050)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pX=bcolz.open(path+'results/conv_test_feat.dat', mode='r')\n",
    "tst_batches=BcolzArrayIterator(pX, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1245/1246 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "test_pred = bn_model.predict_generator(tst_batches, steps=np.ceil(tst_batches.N/batch_size), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = do_clip(test_pred,0.8840)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = sorted(batches.class_indices, key=batches.class_indices.get)\n",
    "subm_name = path+'results/subm2.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_1.jpg</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_10.jpg</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_100.jpg</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_1000.jpg</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.012889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_100000.jpg</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "      <td>0.012889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              img        c0        c1        c2        c3        c4        c5  \\\n",
       "0       img_1.jpg  0.012889  0.012889  0.012889  0.012889  0.012889  0.884000   \n",
       "1      img_10.jpg  0.012889  0.012889  0.012889  0.012889  0.012889  0.884000   \n",
       "2     img_100.jpg  0.884000  0.012889  0.012889  0.012889  0.012889  0.012889   \n",
       "3    img_1000.jpg  0.012889  0.012889  0.012889  0.012889  0.012889  0.012889   \n",
       "4  img_100000.jpg  0.012889  0.012889  0.012889  0.884000  0.012889  0.012889   \n",
       "\n",
       "         c6        c7        c8        c9  \n",
       "0  0.012889  0.012889  0.012889  0.012889  \n",
       "1  0.012889  0.012889  0.012889  0.012889  \n",
       "2  0.012889  0.012889  0.012889  0.012889  \n",
       "3  0.012889  0.012889  0.884000  0.012889  \n",
       "4  0.012889  0.012889  0.012889  0.012889  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(subm, columns=classes)\n",
    "submission.insert(0, 'img', [a[8:] for a in test_filenames])\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv(subm_name, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/home/hearth/ML/course/deeplearning1/nbs/data/statefarm//results/subm2.gz' target='_blank'>/home/hearth/ML/course/deeplearning1/nbs/data/statefarm//results/subm2.gz</a><br>"
      ],
      "text/plain": [
       "/home/hearth/ML/course/deeplearning1/nbs/data/statefarm/results/subm2.gz"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink(subm_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making submissions, old method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenames = val_batches.filenames\n",
    "expected_labels = val_batches.classes #0 or 1\n",
    "\n",
    "#Round our predictions to 0/1 to generate labels\n",
    "print (filenames[1], probs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = load_array(results_path + 'test_preds.dat')\n",
    "filenames = load_array(results_path + 'filenames.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = vgg.get_batches(test_path, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "filenames = batches.filenames\n",
    "a = []\n",
    "\n",
    "for idx, val in enumerate(filenames):\n",
    "    a.append(val[8:])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = sorted(val_batches.class_indices, key=val_batches.class_indices.get)\n",
    "submission = pd.DataFrame(preds, columns=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (submission)\n",
    "submission['img']=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = submission.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "submission= submission[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.to_csv('firstSubVGG', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = load_array('/home/hearth/ML/course/courses/deeplearning1/nbs/data/dogscats/' + 'test_preds.dat')\n",
    "filenames = load_array('/home/hearth/ML/course/courses/deeplearning1/nbs/data/dogscats/' + 'filenames.dat')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
