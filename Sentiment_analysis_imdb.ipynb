{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "Using imdb dataset http://ai.stanford.edu/~amaas/data/sentiment/ \n",
    "25,000 reviews in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 7003 on context None\n",
      "Mapped name None to device cuda0: GeForce GTX 1080 Ti (0000:01:00.0)\n",
      "/opt/anaconda/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import utils; importlib.reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "current_dir = os.getcwd()\n",
    "LESSON_HOME_DIR = current_dir\n",
    "DATA_HOME_DIR = current_dir+'/data/imdb/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/hearth/ML/course/deeplearning1/nbs/data/imdb\n"
     ]
    }
   ],
   "source": [
    "%cd $DATA_HOME_DIR\n",
    "path = DATA_HOME_DIR\n",
    "model_path = path+'/models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching and loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dsPath = 'imdb_full.pkl'                \n",
    "f = open(dsPath, 'rb')\n",
    "(x_train, labels_train), (x_test, labels_test) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('imdb_word_index.json') as json_data:\n",
    "    w2id = json.load(json_data)\n",
    "id2w = {v: k for k, v in w2id.items()}\n",
    "#''.join([idx2word[o] for o in x_train[o]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this movie is terrible but it has some good effects'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([id2w[o] for o in x_train[13568]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " labels_train[20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As many words appear only once in the whole dataset like most nouns, its useful to reduce the vocab size down to 5000. Since the words are arranged in the frequency they appear in the dataset, this is simple to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_train]\n",
    "test = [np.array([i if i<vocab_size-1 else vocab_size-1 for i in s]) for s in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of lengths of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2493, 10, 237.71364)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [len(i) for i in trn]\n",
    "(max(lengths), min(lengths), sum(lengths) / float(len(lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13568"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.index(min(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make all reviews the same length at 500 using padding (with zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_len = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trn = sequence.pad_sequences(trn, maxlen=seq_len, value = 0)\n",
    "test = sequence.pad_sequences(test, maxlen=seq_len, value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 500)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Simple single layer NN Acc 87.5\n",
    "Embedding layer is used in lieu of one hot encodding the vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    #32 latent factors for each word\n",
    "    Embedding(vocab_size, 32, input_length=seq_len),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               1600100   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,760,201\n",
      "Trainable params: 1,760,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 0s - loss: 0.4941 - acc: 0.7236 - val_loss: 0.2906 - val_acc: 0.8768\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 0s - loss: 0.2077 - acc: 0.9212 - val_loss: 0.3013 - val_acc: 0.8758\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fba8eb13320>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn, labels_train, validation_data=(test, labels_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Convolution network with maxpooling (single layer) Acc 88.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "conv1 = Sequential([\n",
    "    Embedding(vocab_size, 32, input_length=seq_len, dropout=0.2),\n",
    "    #Drop above turns off some of the latent factors while\n",
    "    #the one below turns off whole words\n",
    "    Dropout(0.2),\n",
    "    Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 500, 32)           160000    \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 500, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 500, 64)           10304     \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 500, 64)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 250, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 16000)             0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 100)               1600100   \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,770,505\n",
      "Trainable params: 1,770,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv1.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "conv1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 1s - loss: 0.2062 - acc: 0.9256 - val_loss: 0.2658 - val_acc: 0.8908\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 1s - loss: 0.1732 - acc: 0.9389 - val_loss: 0.2899 - val_acc: 0.8873\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fba6c283d30>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1.fit(trn, labels_train, validation_data=(test, labels_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Using pretrained embedding: Glove Dataset \n",
    "Results are consistently better using trained embeddings\n",
    "Dataset: http://files.fast.ai/models/glove/6B.50d.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(loc):\n",
    "    with open(loc+'_words.pkl', 'rb') as f:\n",
    "        d1 = pickle.load(f, encoding='latin1')\n",
    "    with open(loc+'_idx.pkl', 'rb') as f:\n",
    "        d2 = pickle.load(f, encoding='latin1')\n",
    "    return (load_array(loc+'.dat'), d1, d2)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Simple function to create an embedding matrix using the indexes from imdb, and the embeddings from glove (where they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vecs, words, wordidx = load_vectors('6B.50d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_emb():\n",
    "    factors=vecs.shape[1]\n",
    "    emb = np.zeros((vocab_size, factors))\n",
    "    \n",
    "    for i in range(1, len(emb)):\n",
    "        word = id2w[i]\n",
    "        if word and re.match(r\"^[a-zA-Z0-9\\-]*$\", word):\n",
    "            src_id = wordidx[word]\n",
    "            emb[i] = vecs[src_id]\n",
    "        else:\n",
    "            #If the word doesn't exist in glove, randomly init\n",
    "            emb[i] = normal(scale=0.6, size=(factors,))\n",
    "    \n",
    "    #For the rare words id - randomly init as well\n",
    "    emb[-1] = normal(scale=0.6, size=(factors,))\n",
    "    emb/=3\n",
    "\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "conv2 = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, weights=[emb], trainable=True),\n",
    "    Dropout(0.2),\n",
    "    Conv1D(64, 5, padding='same', activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling1D(),\n",
    "    Flatten(),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv2.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      "25000/25000 [==============================] - 2s - loss: 0.0961 - acc: 0.9643 - val_loss: 0.3448 - val_acc: 0.8896\n",
      "Epoch 2/2\n",
      "25000/25000 [==============================] - 2s - loss: 0.0826 - acc: 0.9712 - val_loss: 0.3819 - val_acc: 0.8887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21b0fb4710>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.fit(trn, labels_train, validation_data=(test, labels_test), epochs=2, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv2.optimizer.lr=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 2s - loss: 0.1992 - acc: 0.9258 - val_loss: 0.2602 - val_acc: 0.8944\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 2s - loss: 0.1712 - acc: 0.9368 - val_loss: 0.2582 - val_acc: 0.8968\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 2s - loss: 0.1488 - acc: 0.9448 - val_loss: 0.2835 - val_acc: 0.8935\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 2s - loss: 0.1294 - acc: 0.9516 - val_loss: 0.3026 - val_acc: 0.8929\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 2s - loss: 0.1079 - acc: 0.9609 - val_loss: 0.3111 - val_acc: 0.8902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21b6522080>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2.fit(trn, labels_train, validation_data=(test, labels_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Convolution model using multiple convolution sets of varying filter size Acc 89.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 3, activation=\"relu\", padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 4, activation=\"relu\", padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 5, activation=\"relu\", padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 6, activation=\"relu\", padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(64, 7, activation=\"relu\", padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n",
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:8: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "graph_in = Input ((vocab_size, 50))\n",
    "convs = []\n",
    "for conv_s in range(3, 8):\n",
    "    x = Convolution1D(64, conv_s, border_mode='same', activation='relu')(graph_in)\n",
    "    x = MaxPooling1D()(x)\n",
    "    x = Flatten()(x)\n",
    "    convs.append(x)\n",
    "out = Merge(mode=\"concat\")(convs)\n",
    "graph = Model(graph_in, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "emb = create_emb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:2: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "conv3 = Sequential([\n",
    "    Embedding(vocab_size, 50, input_length=seq_len, dropout=0.2, weights=[emb]),\n",
    "    Dropout(0.2),\n",
    "    graph,\n",
    "    Dropout(0.5),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dropout(0.7),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "conv3.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/5\n",
      "25000/25000 [==============================] - 7s - loss: 0.6328 - acc: 0.6149 - val_loss: 0.3592 - val_acc: 0.8632\n",
      "Epoch 2/5\n",
      "25000/25000 [==============================] - 7s - loss: 0.3420 - acc: 0.8586 - val_loss: 0.2577 - val_acc: 0.8954\n",
      "Epoch 3/5\n",
      "25000/25000 [==============================] - 7s - loss: 0.2607 - acc: 0.9030 - val_loss: 0.2513 - val_acc: 0.8963\n",
      "Epoch 4/5\n",
      "25000/25000 [==============================] - 7s - loss: 0.2185 - acc: 0.9204 - val_loss: 0.2906 - val_acc: 0.8824\n",
      "Epoch 5/5\n",
      "25000/25000 [==============================] - 7s - loss: 0.1938 - acc: 0.9295 - val_loss: 0.2621 - val_acc: 0.8969\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f21a27cc978>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv3.fit(trn, labels_train, validation_data=(test, labels_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
